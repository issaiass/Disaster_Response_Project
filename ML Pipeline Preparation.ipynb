{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Pipeline Preparation\n",
    "Follow the instructions below to help you create your ML pipeline.\n",
    "### 1. Import libraries and load data from database.\n",
    "- Import Python libraries\n",
    "- Load dataset from database with [`read_sql_table`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_table.html)\n",
    "- Define feature and target variables X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sqlalchemy import create_engine\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dowload several nltk packages\n",
    "nltk.download(['punkt', 'wordnet', 'stopwords', 'averaged_perceptron_tagger'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load data from database\n",
    "engine = create_engine('sqlite:///Disaster.db')\n",
    "df = pd.read_sql_table(\"Messages\", engine)\n",
    "X = df.message\n",
    "cols = df.columns[4:]\n",
    "Y = df[cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Write a tokenization function to process your text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"Normalize the data, make a tokenNormalize, tokenize and stem text string\n",
    "    \n",
    "    Input:\n",
    "    text: string. String that contains a message to process\n",
    "       \n",
    "    Output:\n",
    "    normalized: list of strings. List containing a normalized and stemmed token.\n",
    "    \"\"\"    \n",
    "    # Converting everything to lower case\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower())\n",
    "    # Tokenize words\n",
    "    tokens = word_tokenize(text)\n",
    "    # normalization word tokens and remove stop words\n",
    "    normlizer = PorterStemmer()\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    normalized = [normlizer.stem(word) for word in tokens if word not in stop_words]\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['weather', 'updat', 'cold', 'front', 'cuba', 'could', 'pass', 'haiti']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(X[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Build a machine learning pipeline\n",
    "This machine pipeline should take in the `message` column as input and output classification results on the other 36 categories in the dataset. You may find the [MultiOutputClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.multioutput.MultiOutputClassifier.html) helpful for predicting multiple target variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the first pipeline\n",
    "pipe1 = Pipeline([\n",
    "    ('vect', CountVectorizer(tokenizer = tokenize)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultiOutputClassifier(RandomForestClassifier()))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train pipeline\n",
    "- Split data into train and test sets\n",
    "- Train pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide the dataset in train and test, finally train the model\n",
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.15, random_state=42)\n",
    "pipe1.fit(Xtrain, Ytrain);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Test your model\n",
    "Report the f1 score, precision and recall for each output category of the dataset. You can do this by iterating through the columns and calling sklearn's `classification_report` on each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions and get the test data and target names\n",
    "predictions = pipe1.predict(Xtest)\n",
    "#predictions.shape, Ytest.values.shape, len(Y.columns.values)\n",
    "ground_truth = Ytest.values\n",
    "target_names = Y.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(gt, pred, col_names):\n",
    "    \"\"\"Evaluation Metrics \n",
    "    \n",
    "    Input:\n",
    "    gt: array. Array of dataset ground truth.\n",
    "    pred: array. Array of dataset predictions.\n",
    "    col_names: strig list. Name list of predicted labels.\n",
    "       \n",
    "    Output:\n",
    "    df: dataframe. Contains the same output as classification_report\n",
    "    \"\"\"\n",
    "    metrics = []\n",
    "    \n",
    "    # Calculate evaluation metrics for each set of labels\n",
    "    for i in range(len(col_names)):\n",
    "        acc = accuracy_score(gt[:, i], pred[:, i])\n",
    "        prec = precision_score(gt[:, i], pred[:, i], average='micro')\n",
    "        rec = recall_score(gt[:, i], pred[:, i], average='micro')\n",
    "        f1 = f1_score(gt[:, i], pred[:, i], average='micro')\n",
    "        \n",
    "        metrics.append([acc, prec, rec, f1])\n",
    "    \n",
    "    # Create dataframe containing metrics\n",
    "    metrics = np.array(metrics)\n",
    "    columns = ['Accuracy', 'Precision', 'Recall', 'F1']\n",
    "    metrics_df = pd.DataFrame(data = metrics, index = col_names, columns=columns)\n",
    "      \n",
    "    return metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>related</th>\n",
       "      <td>0.800661</td>\n",
       "      <td>0.800661</td>\n",
       "      <td>0.800661</td>\n",
       "      <td>0.800661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>request</th>\n",
       "      <td>0.884058</td>\n",
       "      <td>0.884058</td>\n",
       "      <td>0.884058</td>\n",
       "      <td>0.884058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>offer</th>\n",
       "      <td>0.994152</td>\n",
       "      <td>0.994152</td>\n",
       "      <td>0.994152</td>\n",
       "      <td>0.994152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aid_related</th>\n",
       "      <td>0.744470</td>\n",
       "      <td>0.744470</td>\n",
       "      <td>0.744470</td>\n",
       "      <td>0.744470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>medical_help</th>\n",
       "      <td>0.926519</td>\n",
       "      <td>0.926519</td>\n",
       "      <td>0.926519</td>\n",
       "      <td>0.926519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>medical_products</th>\n",
       "      <td>0.953471</td>\n",
       "      <td>0.953471</td>\n",
       "      <td>0.953471</td>\n",
       "      <td>0.953471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>search_and_rescue</th>\n",
       "      <td>0.975845</td>\n",
       "      <td>0.975845</td>\n",
       "      <td>0.975845</td>\n",
       "      <td>0.975845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>security</th>\n",
       "      <td>0.982456</td>\n",
       "      <td>0.982456</td>\n",
       "      <td>0.982456</td>\n",
       "      <td>0.982456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>military</th>\n",
       "      <td>0.969489</td>\n",
       "      <td>0.969489</td>\n",
       "      <td>0.969489</td>\n",
       "      <td>0.969489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>child_alone</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>water</th>\n",
       "      <td>0.948385</td>\n",
       "      <td>0.948385</td>\n",
       "      <td>0.948385</td>\n",
       "      <td>0.948385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>food</th>\n",
       "      <td>0.927536</td>\n",
       "      <td>0.927536</td>\n",
       "      <td>0.927536</td>\n",
       "      <td>0.927536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shelter</th>\n",
       "      <td>0.935927</td>\n",
       "      <td>0.935927</td>\n",
       "      <td>0.935927</td>\n",
       "      <td>0.935927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>clothing</th>\n",
       "      <td>0.988050</td>\n",
       "      <td>0.988050</td>\n",
       "      <td>0.988050</td>\n",
       "      <td>0.988050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>money</th>\n",
       "      <td>0.982456</td>\n",
       "      <td>0.982456</td>\n",
       "      <td>0.982456</td>\n",
       "      <td>0.982456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>missing_people</th>\n",
       "      <td>0.988304</td>\n",
       "      <td>0.988304</td>\n",
       "      <td>0.988304</td>\n",
       "      <td>0.988304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>refugees</th>\n",
       "      <td>0.968980</td>\n",
       "      <td>0.968980</td>\n",
       "      <td>0.968980</td>\n",
       "      <td>0.968980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>death</th>\n",
       "      <td>0.960844</td>\n",
       "      <td>0.960844</td>\n",
       "      <td>0.960844</td>\n",
       "      <td>0.960844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>other_aid</th>\n",
       "      <td>0.860666</td>\n",
       "      <td>0.860666</td>\n",
       "      <td>0.860666</td>\n",
       "      <td>0.860666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>infrastructure_related</th>\n",
       "      <td>0.933893</td>\n",
       "      <td>0.933893</td>\n",
       "      <td>0.933893</td>\n",
       "      <td>0.933893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>transport</th>\n",
       "      <td>0.953471</td>\n",
       "      <td>0.953471</td>\n",
       "      <td>0.953471</td>\n",
       "      <td>0.953471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>buildings</th>\n",
       "      <td>0.951437</td>\n",
       "      <td>0.951437</td>\n",
       "      <td>0.951437</td>\n",
       "      <td>0.951437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>electricity</th>\n",
       "      <td>0.976862</td>\n",
       "      <td>0.976862</td>\n",
       "      <td>0.976862</td>\n",
       "      <td>0.976862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tools</th>\n",
       "      <td>0.993644</td>\n",
       "      <td>0.993644</td>\n",
       "      <td>0.993644</td>\n",
       "      <td>0.993644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hospitals</th>\n",
       "      <td>0.990592</td>\n",
       "      <td>0.990592</td>\n",
       "      <td>0.990592</td>\n",
       "      <td>0.990592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shops</th>\n",
       "      <td>0.995932</td>\n",
       "      <td>0.995932</td>\n",
       "      <td>0.995932</td>\n",
       "      <td>0.995932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aid_centers</th>\n",
       "      <td>0.987033</td>\n",
       "      <td>0.987033</td>\n",
       "      <td>0.987033</td>\n",
       "      <td>0.987033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>other_infrastructure</th>\n",
       "      <td>0.953471</td>\n",
       "      <td>0.953471</td>\n",
       "      <td>0.953471</td>\n",
       "      <td>0.953471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weather_related</th>\n",
       "      <td>0.862192</td>\n",
       "      <td>0.862192</td>\n",
       "      <td>0.862192</td>\n",
       "      <td>0.862192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>floods</th>\n",
       "      <td>0.946351</td>\n",
       "      <td>0.946351</td>\n",
       "      <td>0.946351</td>\n",
       "      <td>0.946351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>storm</th>\n",
       "      <td>0.931096</td>\n",
       "      <td>0.931096</td>\n",
       "      <td>0.931096</td>\n",
       "      <td>0.931096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fire</th>\n",
       "      <td>0.989321</td>\n",
       "      <td>0.989321</td>\n",
       "      <td>0.989321</td>\n",
       "      <td>0.989321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>earthquake</th>\n",
       "      <td>0.964404</td>\n",
       "      <td>0.964404</td>\n",
       "      <td>0.964404</td>\n",
       "      <td>0.964404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cold</th>\n",
       "      <td>0.979405</td>\n",
       "      <td>0.979405</td>\n",
       "      <td>0.979405</td>\n",
       "      <td>0.979405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>other_weather</th>\n",
       "      <td>0.945080</td>\n",
       "      <td>0.945080</td>\n",
       "      <td>0.945080</td>\n",
       "      <td>0.945080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>direct_report</th>\n",
       "      <td>0.847699</td>\n",
       "      <td>0.847699</td>\n",
       "      <td>0.847699</td>\n",
       "      <td>0.847699</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Accuracy  Precision    Recall        F1\n",
       "related                 0.800661   0.800661  0.800661  0.800661\n",
       "request                 0.884058   0.884058  0.884058  0.884058\n",
       "offer                   0.994152   0.994152  0.994152  0.994152\n",
       "aid_related             0.744470   0.744470  0.744470  0.744470\n",
       "medical_help            0.926519   0.926519  0.926519  0.926519\n",
       "medical_products        0.953471   0.953471  0.953471  0.953471\n",
       "search_and_rescue       0.975845   0.975845  0.975845  0.975845\n",
       "security                0.982456   0.982456  0.982456  0.982456\n",
       "military                0.969489   0.969489  0.969489  0.969489\n",
       "child_alone             1.000000   1.000000  1.000000  1.000000\n",
       "water                   0.948385   0.948385  0.948385  0.948385\n",
       "food                    0.927536   0.927536  0.927536  0.927536\n",
       "shelter                 0.935927   0.935927  0.935927  0.935927\n",
       "clothing                0.988050   0.988050  0.988050  0.988050\n",
       "money                   0.982456   0.982456  0.982456  0.982456\n",
       "missing_people          0.988304   0.988304  0.988304  0.988304\n",
       "refugees                0.968980   0.968980  0.968980  0.968980\n",
       "death                   0.960844   0.960844  0.960844  0.960844\n",
       "other_aid               0.860666   0.860666  0.860666  0.860666\n",
       "infrastructure_related  0.933893   0.933893  0.933893  0.933893\n",
       "transport               0.953471   0.953471  0.953471  0.953471\n",
       "buildings               0.951437   0.951437  0.951437  0.951437\n",
       "electricity             0.976862   0.976862  0.976862  0.976862\n",
       "tools                   0.993644   0.993644  0.993644  0.993644\n",
       "hospitals               0.990592   0.990592  0.990592  0.990592\n",
       "shops                   0.995932   0.995932  0.995932  0.995932\n",
       "aid_centers             0.987033   0.987033  0.987033  0.987033\n",
       "other_infrastructure    0.953471   0.953471  0.953471  0.953471\n",
       "weather_related         0.862192   0.862192  0.862192  0.862192\n",
       "floods                  0.946351   0.946351  0.946351  0.946351\n",
       "storm                   0.931096   0.931096  0.931096  0.931096\n",
       "fire                    0.989321   0.989321  0.989321  0.989321\n",
       "earthquake              0.964404   0.964404  0.964404  0.964404\n",
       "cold                    0.979405   0.979405  0.979405  0.979405\n",
       "other_weather           0.945080   0.945080  0.945080  0.945080\n",
       "direct_report           0.847699   0.847699  0.847699  0.847699"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_metrics(ground_truth, predictions, target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Improve your model\n",
    "Use grid search to find better parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('vect',\n",
       "   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=<function tokenize at 0x7f76f94b30d0>, vocabulary=None)),\n",
       "  ('tfidf',\n",
       "   TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)),\n",
       "  ('clf',\n",
       "   MultiOutputClassifier(estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "               max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "               min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "               min_samples_leaf=1, min_samples_split=2,\n",
       "               min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "               oob_score=False, random_state=None, verbose=0,\n",
       "               warm_start=False),\n",
       "              n_jobs=1))],\n",
       " 'vect': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=<function tokenize at 0x7f76f94b30d0>, vocabulary=None),\n",
       " 'tfidf': TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True),\n",
       " 'clf': MultiOutputClassifier(estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "             max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "             min_samples_leaf=1, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "             oob_score=False, random_state=None, verbose=0,\n",
       "             warm_start=False),\n",
       "            n_jobs=1),\n",
       " 'vect__analyzer': 'word',\n",
       " 'vect__binary': False,\n",
       " 'vect__decode_error': 'strict',\n",
       " 'vect__dtype': numpy.int64,\n",
       " 'vect__encoding': 'utf-8',\n",
       " 'vect__input': 'content',\n",
       " 'vect__lowercase': True,\n",
       " 'vect__max_df': 1.0,\n",
       " 'vect__max_features': None,\n",
       " 'vect__min_df': 1,\n",
       " 'vect__ngram_range': (1, 1),\n",
       " 'vect__preprocessor': None,\n",
       " 'vect__stop_words': None,\n",
       " 'vect__strip_accents': None,\n",
       " 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'vect__tokenizer': <function __main__.tokenize(text)>,\n",
       " 'vect__vocabulary': None,\n",
       " 'tfidf__norm': 'l2',\n",
       " 'tfidf__smooth_idf': True,\n",
       " 'tfidf__sublinear_tf': False,\n",
       " 'tfidf__use_idf': True,\n",
       " 'clf__estimator__bootstrap': True,\n",
       " 'clf__estimator__class_weight': None,\n",
       " 'clf__estimator__criterion': 'gini',\n",
       " 'clf__estimator__max_depth': None,\n",
       " 'clf__estimator__max_features': 'auto',\n",
       " 'clf__estimator__max_leaf_nodes': None,\n",
       " 'clf__estimator__min_impurity_decrease': 0.0,\n",
       " 'clf__estimator__min_impurity_split': None,\n",
       " 'clf__estimator__min_samples_leaf': 1,\n",
       " 'clf__estimator__min_samples_split': 2,\n",
       " 'clf__estimator__min_weight_fraction_leaf': 0.0,\n",
       " 'clf__estimator__n_estimators': 10,\n",
       " 'clf__estimator__n_jobs': 1,\n",
       " 'clf__estimator__oob_score': False,\n",
       " 'clf__estimator__random_state': None,\n",
       " 'clf__estimator__verbose': 0,\n",
       " 'clf__estimator__warm_start': False,\n",
       " 'clf__estimator': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "             max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "             min_samples_leaf=1, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "             oob_score=False, random_state=None, verbose=0,\n",
       "             warm_start=False),\n",
       " 'clf__n_jobs': 1}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's see the model parameters\n",
    "pipe1.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play with the model parameters and make a grid search\n",
    "parameters = {\n",
    "        'vect__ngram_range': ((1, 1), (1, 2)),\n",
    "        #'vector__min_df':[1, 5],\n",
    "        #'vector__max_df': (0.5, 0.75, 1.0),\n",
    "        #'vector__max_features': (None, 5000, 10000),\n",
    "        'tfidf__use_idf': [True, False],\n",
    "        #'clf__estimator__n_estimators': [5, 10, 15],\n",
    "        'clf__estimator__min_samples_split': [2, 5]\n",
    "    }\n",
    "\n",
    "cv = GridSearchCV(pipe1, param_grid=parameters, verbose=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "[CV] clf__estimator__min_samples_split=2, tfidf__use_idf=True, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__estimator__min_samples_split=2, tfidf__use_idf=True, vect__ngram_range=(1, 1), score=0.23411416262789445, total= 1.1min\n",
      "[CV] clf__estimator__min_samples_split=2, tfidf__use_idf=True, vect__ngram_range=(1, 1) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.4min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__min_samples_split=2, tfidf__use_idf=True, vect__ngram_range=(1, 1), score=0.2425955842757135, total= 1.1min\n",
      "[CV] clf__estimator__min_samples_split=2, tfidf__use_idf=True, vect__ngram_range=(1, 1) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  2.9min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__min_samples_split=2, tfidf__use_idf=True, vect__ngram_range=(1, 1), score=0.24208967281540325, total= 1.1min\n",
      "[CV] clf__estimator__min_samples_split=2, tfidf__use_idf=True, vect__ngram_range=(1, 2) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:  4.3min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__min_samples_split=2, tfidf__use_idf=True, vect__ngram_range=(1, 2), score=0.2451534733441034, total= 2.5min\n",
      "[CV] clf__estimator__min_samples_split=2, tfidf__use_idf=True, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__min_samples_split=2, tfidf__use_idf=True, vect__ngram_range=(1, 2), score=0.250942380183091, total= 2.6min\n",
      "[CV] clf__estimator__min_samples_split=2, tfidf__use_idf=True, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__min_samples_split=2, tfidf__use_idf=True, vect__ngram_range=(1, 2), score=0.24747542749427764, total= 2.5min\n",
      "[CV] clf__estimator__min_samples_split=2, tfidf__use_idf=False, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__estimator__min_samples_split=2, tfidf__use_idf=False, vect__ngram_range=(1, 1), score=0.24111470113085623, total= 1.1min\n",
      "[CV] clf__estimator__min_samples_split=2, tfidf__use_idf=False, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__estimator__min_samples_split=2, tfidf__use_idf=False, vect__ngram_range=(1, 1), score=0.23303715670436187, total= 1.1min\n",
      "[CV] clf__estimator__min_samples_split=2, tfidf__use_idf=False, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__estimator__min_samples_split=2, tfidf__use_idf=False, vect__ngram_range=(1, 1), score=0.2428975360172344, total= 1.1min\n",
      "[CV] clf__estimator__min_samples_split=2, tfidf__use_idf=False, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__min_samples_split=2, tfidf__use_idf=False, vect__ngram_range=(1, 2), score=0.23815293484114164, total= 2.8min\n",
      "[CV] clf__estimator__min_samples_split=2, tfidf__use_idf=False, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__min_samples_split=2, tfidf__use_idf=False, vect__ngram_range=(1, 2), score=0.23855681206246634, total= 2.7min\n",
      "[CV] clf__estimator__min_samples_split=2, tfidf__use_idf=False, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__min_samples_split=2, tfidf__use_idf=False, vect__ngram_range=(1, 2), score=0.2482832906961088, total= 2.7min\n",
      "[CV] clf__estimator__min_samples_split=5, tfidf__use_idf=True, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__estimator__min_samples_split=5, tfidf__use_idf=True, vect__ngram_range=(1, 1), score=0.232364028002154, total=  57.1s\n",
      "[CV] clf__estimator__min_samples_split=5, tfidf__use_idf=True, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__estimator__min_samples_split=5, tfidf__use_idf=True, vect__ngram_range=(1, 1), score=0.23128702207862142, total=  57.4s\n",
      "[CV] clf__estimator__min_samples_split=5, tfidf__use_idf=True, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__estimator__min_samples_split=5, tfidf__use_idf=True, vect__ngram_range=(1, 1), score=0.2399353709438535, total=  58.5s\n",
      "[CV] clf__estimator__min_samples_split=5, tfidf__use_idf=True, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__min_samples_split=5, tfidf__use_idf=True, vect__ngram_range=(1, 2), score=0.2403069466882068, total= 2.0min\n",
      "[CV] clf__estimator__min_samples_split=5, tfidf__use_idf=True, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__min_samples_split=5, tfidf__use_idf=True, vect__ngram_range=(1, 2), score=0.2416532040926225, total= 2.0min\n",
      "[CV] clf__estimator__min_samples_split=5, tfidf__use_idf=True, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__min_samples_split=5, tfidf__use_idf=True, vect__ngram_range=(1, 2), score=0.2385889322741349, total= 2.0min\n",
      "[CV] clf__estimator__min_samples_split=5, tfidf__use_idf=False, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__estimator__min_samples_split=5, tfidf__use_idf=False, vect__ngram_range=(1, 1), score=0.23801830910070004, total=  57.0s\n",
      "[CV] clf__estimator__min_samples_split=5, tfidf__use_idf=False, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__estimator__min_samples_split=5, tfidf__use_idf=False, vect__ngram_range=(1, 1), score=0.2319601507808293, total=  57.2s\n",
      "[CV] clf__estimator__min_samples_split=5, tfidf__use_idf=False, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__estimator__min_samples_split=5, tfidf__use_idf=False, vect__ngram_range=(1, 1), score=0.23926215160899422, total=  58.2s\n",
      "[CV] clf__estimator__min_samples_split=5, tfidf__use_idf=False, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__min_samples_split=5, tfidf__use_idf=False, vect__ngram_range=(1, 2), score=0.24017232094776522, total= 2.0min\n",
      "[CV] clf__estimator__min_samples_split=5, tfidf__use_idf=False, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__min_samples_split=5, tfidf__use_idf=False, vect__ngram_range=(1, 2), score=0.23626817447495962, total= 2.0min\n",
      "[CV] clf__estimator__min_samples_split=5, tfidf__use_idf=False, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__min_samples_split=5, tfidf__use_idf=False, vect__ngram_range=(1, 2), score=0.24357075535209372, total= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  24 out of  24 | elapsed: 49.1min finished\n"
     ]
    }
   ],
   "source": [
    "# Train a random forest multiclass classifier\n",
    "model_randomforest = cv.fit(Xtrain, Ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf__estimator__min_samples_split': 2,\n",
       " 'tfidf__use_idf': True,\n",
       " 'vect__ngram_range': (1, 2)}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the best model results\n",
    "model_randomforest.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Test your model\n",
    "Show the accuracy, precision, and recall of the tuned model.  \n",
    "\n",
    "Since this project focuses on code quality, process, and  pipelines, there is no minimum performance metric needed to pass. However, make sure to fine tune your models for accuracy, precision and recall to make your project stand out - especially for your portfolio!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>related</th>\n",
       "      <td>0.791508</td>\n",
       "      <td>0.791508</td>\n",
       "      <td>0.791508</td>\n",
       "      <td>0.791508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>request</th>\n",
       "      <td>0.883804</td>\n",
       "      <td>0.883804</td>\n",
       "      <td>0.883804</td>\n",
       "      <td>0.883804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>offer</th>\n",
       "      <td>0.994152</td>\n",
       "      <td>0.994152</td>\n",
       "      <td>0.994152</td>\n",
       "      <td>0.994152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aid_related</th>\n",
       "      <td>0.743199</td>\n",
       "      <td>0.743199</td>\n",
       "      <td>0.743199</td>\n",
       "      <td>0.743199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>medical_help</th>\n",
       "      <td>0.922197</td>\n",
       "      <td>0.922197</td>\n",
       "      <td>0.922197</td>\n",
       "      <td>0.922197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>medical_products</th>\n",
       "      <td>0.951437</td>\n",
       "      <td>0.951437</td>\n",
       "      <td>0.951437</td>\n",
       "      <td>0.951437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>search_and_rescue</th>\n",
       "      <td>0.977371</td>\n",
       "      <td>0.977371</td>\n",
       "      <td>0.977371</td>\n",
       "      <td>0.977371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>security</th>\n",
       "      <td>0.982202</td>\n",
       "      <td>0.982202</td>\n",
       "      <td>0.982202</td>\n",
       "      <td>0.982202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>military</th>\n",
       "      <td>0.966946</td>\n",
       "      <td>0.966946</td>\n",
       "      <td>0.966946</td>\n",
       "      <td>0.966946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>child_alone</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>water</th>\n",
       "      <td>0.950674</td>\n",
       "      <td>0.950674</td>\n",
       "      <td>0.950674</td>\n",
       "      <td>0.950674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>food</th>\n",
       "      <td>0.929316</td>\n",
       "      <td>0.929316</td>\n",
       "      <td>0.929316</td>\n",
       "      <td>0.929316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shelter</th>\n",
       "      <td>0.929570</td>\n",
       "      <td>0.929570</td>\n",
       "      <td>0.929570</td>\n",
       "      <td>0.929570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>clothing</th>\n",
       "      <td>0.986270</td>\n",
       "      <td>0.986270</td>\n",
       "      <td>0.986270</td>\n",
       "      <td>0.986270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>money</th>\n",
       "      <td>0.981185</td>\n",
       "      <td>0.981185</td>\n",
       "      <td>0.981185</td>\n",
       "      <td>0.981185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>missing_people</th>\n",
       "      <td>0.988050</td>\n",
       "      <td>0.988050</td>\n",
       "      <td>0.988050</td>\n",
       "      <td>0.988050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>refugees</th>\n",
       "      <td>0.966438</td>\n",
       "      <td>0.966438</td>\n",
       "      <td>0.966438</td>\n",
       "      <td>0.966438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>death</th>\n",
       "      <td>0.959319</td>\n",
       "      <td>0.959319</td>\n",
       "      <td>0.959319</td>\n",
       "      <td>0.959319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>other_aid</th>\n",
       "      <td>0.862446</td>\n",
       "      <td>0.862446</td>\n",
       "      <td>0.862446</td>\n",
       "      <td>0.862446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>infrastructure_related</th>\n",
       "      <td>0.935164</td>\n",
       "      <td>0.935164</td>\n",
       "      <td>0.935164</td>\n",
       "      <td>0.935164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>transport</th>\n",
       "      <td>0.952962</td>\n",
       "      <td>0.952962</td>\n",
       "      <td>0.952962</td>\n",
       "      <td>0.952962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>buildings</th>\n",
       "      <td>0.949657</td>\n",
       "      <td>0.949657</td>\n",
       "      <td>0.949657</td>\n",
       "      <td>0.949657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>electricity</th>\n",
       "      <td>0.975591</td>\n",
       "      <td>0.975591</td>\n",
       "      <td>0.975591</td>\n",
       "      <td>0.975591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tools</th>\n",
       "      <td>0.993644</td>\n",
       "      <td>0.993644</td>\n",
       "      <td>0.993644</td>\n",
       "      <td>0.993644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hospitals</th>\n",
       "      <td>0.990338</td>\n",
       "      <td>0.990338</td>\n",
       "      <td>0.990338</td>\n",
       "      <td>0.990338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shops</th>\n",
       "      <td>0.996186</td>\n",
       "      <td>0.996186</td>\n",
       "      <td>0.996186</td>\n",
       "      <td>0.996186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aid_centers</th>\n",
       "      <td>0.986779</td>\n",
       "      <td>0.986779</td>\n",
       "      <td>0.986779</td>\n",
       "      <td>0.986779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>other_infrastructure</th>\n",
       "      <td>0.954233</td>\n",
       "      <td>0.954233</td>\n",
       "      <td>0.954233</td>\n",
       "      <td>0.954233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weather_related</th>\n",
       "      <td>0.847445</td>\n",
       "      <td>0.847445</td>\n",
       "      <td>0.847445</td>\n",
       "      <td>0.847445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>floods</th>\n",
       "      <td>0.942029</td>\n",
       "      <td>0.942029</td>\n",
       "      <td>0.942029</td>\n",
       "      <td>0.942029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>storm</th>\n",
       "      <td>0.926265</td>\n",
       "      <td>0.926265</td>\n",
       "      <td>0.926265</td>\n",
       "      <td>0.926265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fire</th>\n",
       "      <td>0.989321</td>\n",
       "      <td>0.989321</td>\n",
       "      <td>0.989321</td>\n",
       "      <td>0.989321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>earthquake</th>\n",
       "      <td>0.962878</td>\n",
       "      <td>0.962878</td>\n",
       "      <td>0.962878</td>\n",
       "      <td>0.962878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cold</th>\n",
       "      <td>0.979151</td>\n",
       "      <td>0.979151</td>\n",
       "      <td>0.979151</td>\n",
       "      <td>0.979151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>other_weather</th>\n",
       "      <td>0.944826</td>\n",
       "      <td>0.944826</td>\n",
       "      <td>0.944826</td>\n",
       "      <td>0.944826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>direct_report</th>\n",
       "      <td>0.842614</td>\n",
       "      <td>0.842614</td>\n",
       "      <td>0.842614</td>\n",
       "      <td>0.842614</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Accuracy  Precision    Recall        F1\n",
       "related                 0.791508   0.791508  0.791508  0.791508\n",
       "request                 0.883804   0.883804  0.883804  0.883804\n",
       "offer                   0.994152   0.994152  0.994152  0.994152\n",
       "aid_related             0.743199   0.743199  0.743199  0.743199\n",
       "medical_help            0.922197   0.922197  0.922197  0.922197\n",
       "medical_products        0.951437   0.951437  0.951437  0.951437\n",
       "search_and_rescue       0.977371   0.977371  0.977371  0.977371\n",
       "security                0.982202   0.982202  0.982202  0.982202\n",
       "military                0.966946   0.966946  0.966946  0.966946\n",
       "child_alone             1.000000   1.000000  1.000000  1.000000\n",
       "water                   0.950674   0.950674  0.950674  0.950674\n",
       "food                    0.929316   0.929316  0.929316  0.929316\n",
       "shelter                 0.929570   0.929570  0.929570  0.929570\n",
       "clothing                0.986270   0.986270  0.986270  0.986270\n",
       "money                   0.981185   0.981185  0.981185  0.981185\n",
       "missing_people          0.988050   0.988050  0.988050  0.988050\n",
       "refugees                0.966438   0.966438  0.966438  0.966438\n",
       "death                   0.959319   0.959319  0.959319  0.959319\n",
       "other_aid               0.862446   0.862446  0.862446  0.862446\n",
       "infrastructure_related  0.935164   0.935164  0.935164  0.935164\n",
       "transport               0.952962   0.952962  0.952962  0.952962\n",
       "buildings               0.949657   0.949657  0.949657  0.949657\n",
       "electricity             0.975591   0.975591  0.975591  0.975591\n",
       "tools                   0.993644   0.993644  0.993644  0.993644\n",
       "hospitals               0.990338   0.990338  0.990338  0.990338\n",
       "shops                   0.996186   0.996186  0.996186  0.996186\n",
       "aid_centers             0.986779   0.986779  0.986779  0.986779\n",
       "other_infrastructure    0.954233   0.954233  0.954233  0.954233\n",
       "weather_related         0.847445   0.847445  0.847445  0.847445\n",
       "floods                  0.942029   0.942029  0.942029  0.942029\n",
       "storm                   0.926265   0.926265  0.926265  0.926265\n",
       "fire                    0.989321   0.989321  0.989321  0.989321\n",
       "earthquake              0.962878   0.962878  0.962878  0.962878\n",
       "cold                    0.979151   0.979151  0.979151  0.979151\n",
       "other_weather           0.944826   0.944826  0.944826  0.944826\n",
       "direct_report           0.842614   0.842614  0.842614  0.842614"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the random forest multiclass classifier with the test data\n",
    "predictions = model_randomforest.predict(Xtest)\n",
    "ground_truth = Ytest.values\n",
    "target_names = Ytest.columns.values\n",
    "\n",
    "print_metrics(ground_truth, predictions, target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Try improving your model further. Here are a few ideas:\n",
    "* try other machine learning algorithms\n",
    "* add other features besides the TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the next pipeline with an Adaboost Multiclass Classifier\n",
    "pipe2 = Pipeline([\n",
    "    ('vect', CountVectorizer(tokenizer = tokenize)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultiOutputClassifier(AdaBoostClassifier()))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('vect',\n",
       "   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=<function tokenize at 0x7f76f94b30d0>, vocabulary=None)),\n",
       "  ('tfidf',\n",
       "   TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)),\n",
       "  ('clf',\n",
       "   MultiOutputClassifier(estimator=AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
       "             learning_rate=1.0, n_estimators=50, random_state=None),\n",
       "              n_jobs=1))],\n",
       " 'vect': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=<function tokenize at 0x7f76f94b30d0>, vocabulary=None),\n",
       " 'tfidf': TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True),\n",
       " 'clf': MultiOutputClassifier(estimator=AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
       "           learning_rate=1.0, n_estimators=50, random_state=None),\n",
       "            n_jobs=1),\n",
       " 'vect__analyzer': 'word',\n",
       " 'vect__binary': False,\n",
       " 'vect__decode_error': 'strict',\n",
       " 'vect__dtype': numpy.int64,\n",
       " 'vect__encoding': 'utf-8',\n",
       " 'vect__input': 'content',\n",
       " 'vect__lowercase': True,\n",
       " 'vect__max_df': 1.0,\n",
       " 'vect__max_features': None,\n",
       " 'vect__min_df': 1,\n",
       " 'vect__ngram_range': (1, 1),\n",
       " 'vect__preprocessor': None,\n",
       " 'vect__stop_words': None,\n",
       " 'vect__strip_accents': None,\n",
       " 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'vect__tokenizer': <function __main__.tokenize(text)>,\n",
       " 'vect__vocabulary': None,\n",
       " 'tfidf__norm': 'l2',\n",
       " 'tfidf__smooth_idf': True,\n",
       " 'tfidf__sublinear_tf': False,\n",
       " 'tfidf__use_idf': True,\n",
       " 'clf__estimator__algorithm': 'SAMME.R',\n",
       " 'clf__estimator__base_estimator': None,\n",
       " 'clf__estimator__learning_rate': 1.0,\n",
       " 'clf__estimator__n_estimators': 50,\n",
       " 'clf__estimator__random_state': None,\n",
       " 'clf__estimator': AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
       "           learning_rate=1.0, n_estimators=50, random_state=None),\n",
       " 'clf__n_jobs': 1}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the parameters of this model\n",
    "pipe2.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n",
      "[CV] clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__ngram_range=(1, 1), score=0.1894184168012924, total=  39.9s\n",
      "[CV] clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__ngram_range=(1, 1) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   57.7s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__ngram_range=(1, 1), score=0.19978459881529348, total=  40.5s\n",
      "[CV] clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__ngram_range=(1, 1) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  1.9min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__ngram_range=(1, 1), score=0.19671468964588662, total=  40.8s\n",
      "[CV] clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__ngram_range=(1, 2) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:  2.9min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__ngram_range=(1, 2), score=0.19116855142703285, total= 1.2min\n",
      "[CV] clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__ngram_range=(1, 2), score=0.19547657512116318, total= 1.2min\n",
      "[CV] clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_estimators=10, tfidf__use_idf=True, vect__ngram_range=(1, 2), score=0.19065571563215294, total= 1.2min\n",
      "[CV] clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__ngram_range=(1, 1), score=0.19184168012924072, total=  38.8s\n",
      "[CV] clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__ngram_range=(1, 1), score=0.20180398492191706, total=  38.7s\n",
      "[CV] clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__ngram_range=(1, 1), score=0.19819577218257708, total=  39.6s\n",
      "[CV] clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__ngram_range=(1, 2), score=0.19184168012924072, total= 1.2min\n",
      "[CV] clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__ngram_range=(1, 2), score=0.20059235325794292, total= 1.1min\n",
      "[CV] clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_estimators=10, tfidf__use_idf=False, vect__ngram_range=(1, 2), score=0.1950989632422243, total= 1.2min\n",
      "[CV] clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__ngram_range=(1, 1), score=0.20665051157781367, total= 1.1min\n",
      "[CV] clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__ngram_range=(1, 1), score=0.21297792137856758, total= 1.1min\n",
      "[CV] clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__ngram_range=(1, 1), score=0.209775144742157, total= 1.1min\n",
      "[CV] clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__ngram_range=(1, 2), score=0.20436187399030695, total= 2.4min\n",
      "[CV] clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__ngram_range=(1, 2), score=0.2143241787829833, total= 2.4min\n",
      "[CV] clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_estimators=25, tfidf__use_idf=True, vect__ngram_range=(1, 2), score=0.20654369193483238, total= 2.4min\n",
      "[CV] clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__ngram_range=(1, 1), score=0.2026117393645665, total=  59.4s\n",
      "[CV] clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__ngram_range=(1, 1), score=0.21015078082929456, total= 1.0min\n",
      "[CV] clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__ngram_range=(1, 1), score=0.2068129796687761, total=  59.9s\n",
      "[CV] clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__ngram_range=(1, 2), score=0.20274636510500807, total= 2.3min\n",
      "[CV] clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__ngram_range=(1, 2), score=0.21122778675282713, total= 2.2min\n",
      "[CV] clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_estimators=25, tfidf__use_idf=False, vect__ngram_range=(1, 2), score=0.21044836407701628, total= 2.2min\n",
      "[CV] clf__estimator__n_estimators=50, tfidf__use_idf=True, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__estimator__n_estimators=50, tfidf__use_idf=True, vect__ngram_range=(1, 1), score=0.20597738287560582, total= 1.7min\n",
      "[CV] clf__estimator__n_estimators=50, tfidf__use_idf=True, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__estimator__n_estimators=50, tfidf__use_idf=True, vect__ngram_range=(1, 1), score=0.21203554119547657, total= 1.7min\n",
      "[CV] clf__estimator__n_estimators=50, tfidf__use_idf=True, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__estimator__n_estimators=50, tfidf__use_idf=True, vect__ngram_range=(1, 1), score=0.22687491584758315, total= 1.7min\n",
      "[CV] clf__estimator__n_estimators=50, tfidf__use_idf=True, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_estimators=50, tfidf__use_idf=True, vect__ngram_range=(1, 2), score=0.20153473344103393, total= 3.9min\n",
      "[CV] clf__estimator__n_estimators=50, tfidf__use_idf=True, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_estimators=50, tfidf__use_idf=True, vect__ngram_range=(1, 2), score=0.21647819063004847, total= 3.9min\n",
      "[CV] clf__estimator__n_estimators=50, tfidf__use_idf=True, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_estimators=50, tfidf__use_idf=True, vect__ngram_range=(1, 2), score=0.20950585700821328, total= 3.9min\n",
      "[CV] clf__estimator__n_estimators=50, tfidf__use_idf=False, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__estimator__n_estimators=50, tfidf__use_idf=False, vect__ngram_range=(1, 1), score=0.2079967689822294, total= 1.6min\n",
      "[CV] clf__estimator__n_estimators=50, tfidf__use_idf=False, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__estimator__n_estimators=50, tfidf__use_idf=False, vect__ngram_range=(1, 1), score=0.21351642434033388, total= 1.6min\n",
      "[CV] clf__estimator__n_estimators=50, tfidf__use_idf=False, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__estimator__n_estimators=50, tfidf__use_idf=False, vect__ngram_range=(1, 1), score=0.22081594183384948, total= 1.6min\n",
      "[CV] clf__estimator__n_estimators=50, tfidf__use_idf=False, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_estimators=50, tfidf__use_idf=False, vect__ngram_range=(1, 2), score=0.2070543887991384, total= 3.6min\n",
      "[CV] clf__estimator__n_estimators=50, tfidf__use_idf=False, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_estimators=50, tfidf__use_idf=False, vect__ngram_range=(1, 2), score=0.21082390953150243, total= 3.7min\n",
      "[CV] clf__estimator__n_estimators=50, tfidf__use_idf=False, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__n_estimators=50, tfidf__use_idf=False, vect__ngram_range=(1, 2), score=0.2240473946411741, total= 3.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  36 out of  36 | elapsed: 76.2min finished\n"
     ]
    }
   ],
   "source": [
    "# get the parameters and made a grid search, finally train the adaboost multiclass classifier\n",
    "parameters = {\n",
    "        'vect__ngram_range': ((1,1), (1,2)),\n",
    "        'tfidf__use_idf': [True, False],\n",
    "        'clf__estimator__n_estimators': [10, 25, 50],\n",
    "    }\n",
    "\n",
    "cv = GridSearchCV(pipe2, param_grid=parameters, verbose=4)\n",
    "model_adaboost = cv.fit(Xtrain, Ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf__estimator__n_estimators': 50,\n",
       " 'tfidf__use_idf': True,\n",
       " 'vect__ngram_range': (1, 1)}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the best parameters\n",
    "model_adaboost.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>related</th>\n",
       "      <td>0.769641</td>\n",
       "      <td>0.769641</td>\n",
       "      <td>0.769641</td>\n",
       "      <td>0.769641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>request</th>\n",
       "      <td>0.887109</td>\n",
       "      <td>0.887109</td>\n",
       "      <td>0.887109</td>\n",
       "      <td>0.887109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>offer</th>\n",
       "      <td>0.993135</td>\n",
       "      <td>0.993135</td>\n",
       "      <td>0.993135</td>\n",
       "      <td>0.993135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aid_related</th>\n",
       "      <td>0.754132</td>\n",
       "      <td>0.754132</td>\n",
       "      <td>0.754132</td>\n",
       "      <td>0.754132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>medical_help</th>\n",
       "      <td>0.925502</td>\n",
       "      <td>0.925502</td>\n",
       "      <td>0.925502</td>\n",
       "      <td>0.925502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>medical_products</th>\n",
       "      <td>0.956776</td>\n",
       "      <td>0.956776</td>\n",
       "      <td>0.956776</td>\n",
       "      <td>0.956776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>search_and_rescue</th>\n",
       "      <td>0.977117</td>\n",
       "      <td>0.977117</td>\n",
       "      <td>0.977117</td>\n",
       "      <td>0.977117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>security</th>\n",
       "      <td>0.980676</td>\n",
       "      <td>0.980676</td>\n",
       "      <td>0.980676</td>\n",
       "      <td>0.980676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>military</th>\n",
       "      <td>0.968980</td>\n",
       "      <td>0.968980</td>\n",
       "      <td>0.968980</td>\n",
       "      <td>0.968980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>child_alone</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>water</th>\n",
       "      <td>0.967455</td>\n",
       "      <td>0.967455</td>\n",
       "      <td>0.967455</td>\n",
       "      <td>0.967455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>food</th>\n",
       "      <td>0.945843</td>\n",
       "      <td>0.945843</td>\n",
       "      <td>0.945843</td>\n",
       "      <td>0.945843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shelter</th>\n",
       "      <td>0.941775</td>\n",
       "      <td>0.941775</td>\n",
       "      <td>0.941775</td>\n",
       "      <td>0.941775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>clothing</th>\n",
       "      <td>0.988813</td>\n",
       "      <td>0.988813</td>\n",
       "      <td>0.988813</td>\n",
       "      <td>0.988813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>money</th>\n",
       "      <td>0.978642</td>\n",
       "      <td>0.978642</td>\n",
       "      <td>0.978642</td>\n",
       "      <td>0.978642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>missing_people</th>\n",
       "      <td>0.988304</td>\n",
       "      <td>0.988304</td>\n",
       "      <td>0.988304</td>\n",
       "      <td>0.988304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>refugees</th>\n",
       "      <td>0.968980</td>\n",
       "      <td>0.968980</td>\n",
       "      <td>0.968980</td>\n",
       "      <td>0.968980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>death</th>\n",
       "      <td>0.965167</td>\n",
       "      <td>0.965167</td>\n",
       "      <td>0.965167</td>\n",
       "      <td>0.965167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>other_aid</th>\n",
       "      <td>0.864989</td>\n",
       "      <td>0.864989</td>\n",
       "      <td>0.864989</td>\n",
       "      <td>0.864989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>infrastructure_related</th>\n",
       "      <td>0.932876</td>\n",
       "      <td>0.932876</td>\n",
       "      <td>0.932876</td>\n",
       "      <td>0.932876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>transport</th>\n",
       "      <td>0.953725</td>\n",
       "      <td>0.953725</td>\n",
       "      <td>0.953725</td>\n",
       "      <td>0.953725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>buildings</th>\n",
       "      <td>0.957793</td>\n",
       "      <td>0.957793</td>\n",
       "      <td>0.957793</td>\n",
       "      <td>0.957793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>electricity</th>\n",
       "      <td>0.978134</td>\n",
       "      <td>0.978134</td>\n",
       "      <td>0.978134</td>\n",
       "      <td>0.978134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tools</th>\n",
       "      <td>0.993135</td>\n",
       "      <td>0.993135</td>\n",
       "      <td>0.993135</td>\n",
       "      <td>0.993135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hospitals</th>\n",
       "      <td>0.989067</td>\n",
       "      <td>0.989067</td>\n",
       "      <td>0.989067</td>\n",
       "      <td>0.989067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shops</th>\n",
       "      <td>0.996186</td>\n",
       "      <td>0.996186</td>\n",
       "      <td>0.996186</td>\n",
       "      <td>0.996186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aid_centers</th>\n",
       "      <td>0.984999</td>\n",
       "      <td>0.984999</td>\n",
       "      <td>0.984999</td>\n",
       "      <td>0.984999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>other_infrastructure</th>\n",
       "      <td>0.950928</td>\n",
       "      <td>0.950928</td>\n",
       "      <td>0.950928</td>\n",
       "      <td>0.950928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weather_related</th>\n",
       "      <td>0.874396</td>\n",
       "      <td>0.874396</td>\n",
       "      <td>0.874396</td>\n",
       "      <td>0.874396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>floods</th>\n",
       "      <td>0.959064</td>\n",
       "      <td>0.959064</td>\n",
       "      <td>0.959064</td>\n",
       "      <td>0.959064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>storm</th>\n",
       "      <td>0.933384</td>\n",
       "      <td>0.933384</td>\n",
       "      <td>0.933384</td>\n",
       "      <td>0.933384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fire</th>\n",
       "      <td>0.989321</td>\n",
       "      <td>0.989321</td>\n",
       "      <td>0.989321</td>\n",
       "      <td>0.989321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>earthquake</th>\n",
       "      <td>0.971014</td>\n",
       "      <td>0.971014</td>\n",
       "      <td>0.971014</td>\n",
       "      <td>0.971014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cold</th>\n",
       "      <td>0.981948</td>\n",
       "      <td>0.981948</td>\n",
       "      <td>0.981948</td>\n",
       "      <td>0.981948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>other_weather</th>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.947368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>direct_report</th>\n",
       "      <td>0.847699</td>\n",
       "      <td>0.847699</td>\n",
       "      <td>0.847699</td>\n",
       "      <td>0.847699</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Accuracy  Precision    Recall        F1\n",
       "related                 0.769641   0.769641  0.769641  0.769641\n",
       "request                 0.887109   0.887109  0.887109  0.887109\n",
       "offer                   0.993135   0.993135  0.993135  0.993135\n",
       "aid_related             0.754132   0.754132  0.754132  0.754132\n",
       "medical_help            0.925502   0.925502  0.925502  0.925502\n",
       "medical_products        0.956776   0.956776  0.956776  0.956776\n",
       "search_and_rescue       0.977117   0.977117  0.977117  0.977117\n",
       "security                0.980676   0.980676  0.980676  0.980676\n",
       "military                0.968980   0.968980  0.968980  0.968980\n",
       "child_alone             1.000000   1.000000  1.000000  1.000000\n",
       "water                   0.967455   0.967455  0.967455  0.967455\n",
       "food                    0.945843   0.945843  0.945843  0.945843\n",
       "shelter                 0.941775   0.941775  0.941775  0.941775\n",
       "clothing                0.988813   0.988813  0.988813  0.988813\n",
       "money                   0.978642   0.978642  0.978642  0.978642\n",
       "missing_people          0.988304   0.988304  0.988304  0.988304\n",
       "refugees                0.968980   0.968980  0.968980  0.968980\n",
       "death                   0.965167   0.965167  0.965167  0.965167\n",
       "other_aid               0.864989   0.864989  0.864989  0.864989\n",
       "infrastructure_related  0.932876   0.932876  0.932876  0.932876\n",
       "transport               0.953725   0.953725  0.953725  0.953725\n",
       "buildings               0.957793   0.957793  0.957793  0.957793\n",
       "electricity             0.978134   0.978134  0.978134  0.978134\n",
       "tools                   0.993135   0.993135  0.993135  0.993135\n",
       "hospitals               0.989067   0.989067  0.989067  0.989067\n",
       "shops                   0.996186   0.996186  0.996186  0.996186\n",
       "aid_centers             0.984999   0.984999  0.984999  0.984999\n",
       "other_infrastructure    0.950928   0.950928  0.950928  0.950928\n",
       "weather_related         0.874396   0.874396  0.874396  0.874396\n",
       "floods                  0.959064   0.959064  0.959064  0.959064\n",
       "storm                   0.933384   0.933384  0.933384  0.933384\n",
       "fire                    0.989321   0.989321  0.989321  0.989321\n",
       "earthquake              0.971014   0.971014  0.971014  0.971014\n",
       "cold                    0.981948   0.981948  0.981948  0.981948\n",
       "other_weather           0.947368   0.947368  0.947368  0.947368\n",
       "direct_report           0.847699   0.847699  0.847699  0.847699"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make the predictions using adaboost\n",
    "predictions = model_adaboost.predict(Xtest)\n",
    "ground_truth = Ytest.values\n",
    "target_names = Ytest.columns.values\n",
    "\n",
    "print_metrics(ground_truth, predictions, target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Export your model as a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best version of the model, this is the case for the random forest\n",
    "filename = 'Response.pkl'\n",
    "pickle.dump(model_randomforest, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Use this notebook to complete `train.py`\n",
    "Use the template file attached in the Resources folder to write a script that runs the steps above to create a database and export a model based on a new dataset specified by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sqlalchemy import create_engine\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(database_filepath):\n",
    "    \"\"\"Load the database to train\n",
    "    \n",
    "    Input:\n",
    "    database_filepath: string. String that contains a database file to read containing the messages\n",
    "       \n",
    "    Output:\n",
    "    X: series.  A set of messages of the database to train\n",
    "    Y: series.  A set of labels of the database to train\n",
    "    category_names: array. List of the 36 classes to classify.\n",
    "    \"\"\"    \n",
    "    nltk.download(['punkt', 'wordnet', 'stopwords', 'averaged_perceptron_tagger'])\n",
    "    engine = create_engine('sqlite:///' + database_filepath)\n",
    "    df = pd.read_sql_table(\"Messages\", engine)\n",
    "    X = df.message\n",
    "    category_names = df.columns[4:]\n",
    "    Y = df[category_names]\n",
    "    return X, Y, category_names\n",
    "        \n",
    "def tokenize(text):\n",
    "    \"\"\"Normalize the data, make a tokenNormalize, tokenize and stem text string\n",
    "    \n",
    "    Input:\n",
    "    text: string. String that contains a message to process\n",
    "       \n",
    "    Output:\n",
    "    normalized: list of strings. List containing a normalized and stemmed token.\n",
    "    \"\"\"    \n",
    "    # Converting everything to lower case\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower())\n",
    "    # Tokenize words\n",
    "    tokens = word_tokenize(text)\n",
    "    # normalization word tokens and remove stop words\n",
    "    normlizer = PorterStemmer()\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    normalized = [normlizer.stem(word) for word in tokens if word not in stop_words]\n",
    "    return normalized\n",
    "    \n",
    "def build_model():\n",
    "    \"\"\"Make the model and do a grid search\n",
    "    \n",
    "    Input:\n",
    "    None\n",
    "       \n",
    "    Output:\n",
    "    model:  The resulting model before the training process, gridsearch set.\n",
    "    \"\"\"    \n",
    "    pipe = Pipeline([\n",
    "        ('vect', CountVectorizer(tokenizer = tokenize)),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('clf', MultiOutputClassifier(RandomForestClassifier()))\n",
    "    ])\n",
    "  \n",
    "    parameters = {\n",
    "        'vect__ngram_range': ((1,1),(1, 2)),\n",
    "        'tfidf__use_idf': [True],\n",
    "        'clf__estimator__min_samples_split': [2, 4]\n",
    "    }    \n",
    "    \n",
    "    model = GridSearchCV(pipe, param_grid=parameters, verbose=4)\n",
    "    return model\n",
    "\n",
    "\n",
    "def __print_metrics(gt, pred, col_names):\n",
    "    \"\"\"Evaluation Metrics \n",
    "    \n",
    "    Input:\n",
    "    gt: array. Array of dataset ground truth.\n",
    "    pred: array. Array of dataset predictions.\n",
    "    col_names: strig list. Name list of predicted labels.\n",
    "       \n",
    "    Output:\n",
    "    df: dataframe. Contains the same output as classification_report\n",
    "    \"\"\"\n",
    "    metrics = []\n",
    "    \n",
    "    # Calculate evaluation metrics for each set of labels\n",
    "    for i in range(len(col_names)):\n",
    "        acc = accuracy_score(gt[:, i], pred[:, i])\n",
    "        prec = precision_score(gt[:, i], pred[:, i], average='micro')\n",
    "        rec = recall_score(gt[:, i], pred[:, i], average='micro')\n",
    "        f1 = f1_score(gt[:, i], pred[:, i], average='micro')\n",
    "        \n",
    "        metrics.append([acc, prec, rec, f1])\n",
    "    \n",
    "    # Create dataframe containing metrics\n",
    "    metrics = np.array(metrics)\n",
    "    columns = ['Accuracy', 'Precision', 'Recall', 'F1']\n",
    "    metrics_df = pd.DataFrame(data = metrics, index = col_names, columns=columns)\n",
    "      \n",
    "    return metrics_df\n",
    "\n",
    "\n",
    "def evaluate_model(model, X_test, Y_test, category_names):\n",
    "    \"\"\"Predict and evaluate some metrics of the model\n",
    "    \n",
    "    Input:\n",
    "    model:  The model of the current data to evaluate\n",
    "    X_test:  series.  The feature values of the test set to evaluate\n",
    "    Y_test:  series.  The label values of the dataset to evaluate \n",
    "    category_names:  list of strings. The array of the labels to evaluate\n",
    "       \n",
    "    Output:\n",
    "    None.  But it displays the printed output of the metrics of the model\n",
    "    \"\"\"    \n",
    "    predictions = model.predict(X_test)\n",
    "    ground_truth = Y_test.values\n",
    "    target_names = category_names #Ytest.columns.values.tolist()\n",
    "    print(__print_metrics(ground_truth, predictions, target_names))\n",
    "    \n",
    "def save_model(model, model_filepath):\n",
    "    \"\"\"Save the final model of the disaster response dataset\n",
    "    \n",
    "    Input:\n",
    "    model: The model to save\n",
    "    model_filepath:  string.  The path of the model to save\n",
    "       \n",
    "    Output:\n",
    "    None\n",
    "    \"\"\"    \n",
    "    pickle.dump(model, open(model_filepath, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "    DATABASE: Disaster.db\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "Building model...\n",
      "Training model...\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "[CV] clf__estimator__min_samples_split=2, tfidf__use_idf=True, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__estimator__min_samples_split=2, tfidf__use_idf=True, vect__ngram_range=(1, 1), score=0.23128702207862142, total= 1.1min\n",
      "[CV] clf__estimator__min_samples_split=2, tfidf__use_idf=True, vect__ngram_range=(1, 1) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.4min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__min_samples_split=2, tfidf__use_idf=True, vect__ngram_range=(1, 1), score=0.2460958535271944, total= 1.1min\n",
      "[CV] clf__estimator__min_samples_split=2, tfidf__use_idf=True, vect__ngram_range=(1, 1) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  2.9min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__min_samples_split=2, tfidf__use_idf=True, vect__ngram_range=(1, 1), score=0.23441497239800727, total= 1.1min\n",
      "[CV] clf__estimator__min_samples_split=2, tfidf__use_idf=True, vect__ngram_range=(1, 2) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:  4.3min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__estimator__min_samples_split=2, tfidf__use_idf=True, vect__ngram_range=(1, 2), score=0.23963381798599892, total= 2.6min\n",
      "[CV] clf__estimator__min_samples_split=2, tfidf__use_idf=True, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__min_samples_split=2, tfidf__use_idf=True, vect__ngram_range=(1, 2), score=0.24138395261173937, total= 2.6min\n",
      "[CV] clf__estimator__min_samples_split=2, tfidf__use_idf=True, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__min_samples_split=2, tfidf__use_idf=True, vect__ngram_range=(1, 2), score=0.2412818096135721, total= 2.6min\n",
      "[CV] clf__estimator__min_samples_split=4, tfidf__use_idf=True, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__estimator__min_samples_split=4, tfidf__use_idf=True, vect__ngram_range=(1, 1), score=0.2358642972536349, total=  59.8s\n",
      "[CV] clf__estimator__min_samples_split=4, tfidf__use_idf=True, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__estimator__min_samples_split=4, tfidf__use_idf=True, vect__ngram_range=(1, 1), score=0.23397953688745288, total=  59.2s\n",
      "[CV] clf__estimator__min_samples_split=4, tfidf__use_idf=True, vect__ngram_range=(1, 1) \n",
      "[CV]  clf__estimator__min_samples_split=4, tfidf__use_idf=True, vect__ngram_range=(1, 1), score=0.2336071091961761, total=  59.0s\n",
      "[CV] clf__estimator__min_samples_split=4, tfidf__use_idf=True, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__min_samples_split=4, tfidf__use_idf=True, vect__ngram_range=(1, 2), score=0.23626817447495962, total= 2.0min\n",
      "[CV] clf__estimator__min_samples_split=4, tfidf__use_idf=True, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__min_samples_split=4, tfidf__use_idf=True, vect__ngram_range=(1, 2), score=0.23465266558966075, total= 2.1min\n",
      "[CV] clf__estimator__min_samples_split=4, tfidf__use_idf=True, vect__ngram_range=(1, 2) \n",
      "[CV]  clf__estimator__min_samples_split=4, tfidf__use_idf=True, vect__ngram_range=(1, 2), score=0.23818500067321932, total= 2.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  12 out of  12 | elapsed: 24.7min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "           n_jobs=1))]),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'vect__ngram_range': ((1, 1), (1, 2)), 'tfidf__use_idf': [True], 'clf__estimator__min_samples_split': [2, 4]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=4)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Proceed to load the data, split, build the model and fit it\n",
    "database_filepath, model_filepath = 'Disaster.db', 'Response.pkl'\n",
    "print('Loading data...\\n    DATABASE: {}'.format(database_filepath))\n",
    "\n",
    "X, Y, category_names = load_data(database_filepath)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.15)\n",
    "\n",
    "print('Building model...')\n",
    "model = build_model()\n",
    "\n",
    "print('Training model...')\n",
    "model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model...\n",
      "                        Accuracy  Precision    Recall        F1\n",
      "related                 0.799644   0.799644  0.799644  0.799644\n",
      "request                 0.887618   0.887618  0.887618  0.887618\n",
      "offer                   0.993898   0.993898  0.993898  0.993898\n",
      "aid_related             0.747267   0.747267  0.747267  0.747267\n",
      "medical_help            0.913298   0.913298  0.913298  0.913298\n",
      "medical_products        0.950928   0.950928  0.950928  0.950928\n",
      "search_and_rescue       0.968218   0.968218  0.968218  0.968218\n",
      "security                0.979405   0.979405  0.979405  0.979405\n",
      "military                0.965929   0.965929  0.965929  0.965929\n",
      "child_alone             1.000000   1.000000  1.000000  1.000000\n",
      "water                   0.951182   0.951182  0.951182  0.951182\n",
      "food                    0.927282   0.927282  0.927282  0.927282\n",
      "shelter                 0.928808   0.928808  0.928808  0.928808\n",
      "clothing                0.984490   0.984490  0.984490  0.984490\n",
      "money                   0.976100   0.976100  0.976100  0.976100\n",
      "missing_people          0.987287   0.987287  0.987287  0.987287\n",
      "refugees                0.967963   0.967963  0.967963  0.967963\n",
      "death                   0.956267   0.956267  0.956267  0.956267\n",
      "other_aid               0.870582   0.870582  0.870582  0.870582\n",
      "infrastructure_related  0.931096   0.931096  0.931096  0.931096\n",
      "transport               0.952454   0.952454  0.952454  0.952454\n",
      "buildings               0.948894   0.948894  0.948894  0.948894\n",
      "electricity             0.983727   0.983727  0.983727  0.983727\n",
      "tools                   0.994152   0.994152  0.994152  0.994152\n",
      "hospitals               0.988304   0.988304  0.988304  0.988304\n",
      "shops                   0.994661   0.994661  0.994661  0.994661\n",
      "aid_centers             0.987541   0.987541  0.987541  0.987541\n",
      "other_infrastructure    0.950928   0.950928  0.950928  0.950928\n",
      "weather_related         0.840580   0.840580  0.840580  0.840580\n",
      "floods                  0.945589   0.945589  0.945589  0.945589\n",
      "storm                   0.928808   0.928808  0.928808  0.928808\n",
      "fire                    0.987796   0.987796  0.987796  0.987796\n",
      "earthquake              0.950928   0.950928  0.950928  0.950928\n",
      "cold                    0.981948   0.981948  0.981948  0.981948\n",
      "other_weather           0.943809   0.943809  0.943809  0.943809\n",
      "direct_report           0.858124   0.858124  0.858124  0.858124\n",
      "Saving model...\n",
      "    MODEL: Response.pkl\n",
      "Trained model saved!\n"
     ]
    }
   ],
   "source": [
    "# Proceed to make evaluaton metrics, save the model and finish\n",
    "print('Evaluating model...')\n",
    "evaluate_model(model, X_test, Y_test, category_names)\n",
    "\n",
    "print('Saving model...\\n    MODEL: {}'.format(model_filepath))\n",
    "save_model(model, model_filepath)\n",
    "\n",
    "print('Trained model saved!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
